
from pyspark.sql.functions import input_file_name, regexp_extract
from pyspark.sql.functions import col, substring
from pyspark.sql import functions as F
from pyspark.sql.functions import collect_list, concat_ws

books_df = spark.read.text("/home/hadoop/spark_books/*.txt")
books_df = books_df.withColumn("file_name", input_file_name())
books_df.show(5, truncate=False)

books_grouped = books_df.groupBy("file_name").agg(concat_ws("\n", collect_list("value")).alias("text"))

books_grouped.show(2, truncate=False)

books_grouped.select(  "file_name", substring("text", 1, 100).alias("text_preview") ).show(2, truncate=False)


books_grouped = books_grouped.withColumn(
    "title",
    F.regexp_extract("text", r"(?im)^Title:\s*(.*)", 1)
).withColumn(
    "release_date",
    F.regexp_extract("text", r"(?im)^Release Date:\s*(.*)", 1)
).withColumn(
    "language",
    F.regexp_extract("text", r"(?im)^Language:\s*(.*)", 1)
).withColumn(
    "encoding",
    F.regexp_extract("text", r"(?im)^Encoding:\s*(.*)", 1)
)

books_grouped.select("file_name", "title", "release_date", "language", "encoding").show(5, truncate=False)
books_grouped = books_grouped.withColumn(
    "release_year",
    F.regexp_extract("release_date", r"(\d{4})", 1)
)

books_grouped.groupBy("release_year") \
    .count() \
    .orderBy("release_year") \
    .show()

books_grouped.groupBy("language") \
    .count() \
    .orderBy(F.desc("count")) \
    .show(1)

books_grouped.withColumn("title_length", F.length("title")) \
    .agg(F.avg("title_length").alias("avg_title_length")) \
    .show()




